## ğŸ“¸ Demo Screenshots

### 1ï¸âƒ£ Model Benchmark Results
Execution accuracy comparison of various models on the Spider test set:
![Comparative Execution Accuracy](Screensho3.png)

### 2ï¸âƒ£ NL2SQL Query Assistant â€“ Home Screen
Main UI for entering natural language questions and retrieving SQL answers:
![NL2SQL UI](Screenshot1.png)

### 3ï¸âƒ£ Sample Query Execution
Example query showing generated SQL and the final answer:
![Query Execution](Screenshot2.png)

# ğŸ§  Natural Language-to-SQL Copilot

Natural Language-to-SQL Copilot helps users convert natural language questions into SQL queries, execute them on real databases, and interpret the results. It removes SQL as a barrier to insights using AI-driven query generation and analysis.

Add demo in future

---

## ğŸš€ Features

- Natural language input â†’ SQL query generation
- Automatic schema recognition via vector search
- Query validation, execution, and result interpretation
- FastAPI backend + Streamlit frontend
- Powered by OpenAI or HuggingFace models (like `flan-t5-xxl`)
- Built on top of the Spider benchmark dataset

---

## ğŸ—‚ï¸ Project Structure

```
NL2SQL/
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ README.md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ backend.py       # FastAPI backend
â”‚   â”‚   â”œâ”€â”€ frontend.py      # Streamlit frontend
â”‚   â”‚   â”œâ”€â”€ main.py          # Core SQL Copilot logic
â”‚   â”‚   â”œâ”€â”€ sql_functions.py # DB logic, embedding, SQL generation
â”‚   â”‚   â””â”€â”€ test.py          # Optional test runner
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ raw/             # Place Spider dataset here
â”‚
â””â”€â”€ venv/                    # Python virtual environment (optional)
```

---

## ğŸ“¦ Installation & Setup

### 1. ğŸ“ Get the Dataset

This project is designed for the [Spider](https://yale-lily.github.io/spider) benchmark dataset.

Download and setup:

```bash
# Step 1: Download
wget --content-disposition "https://drive.google.com/uc?export=download&id=1TqleXec_OykOYFREKKtschzY29dUcVAQ"

# Step 2: Unzip
unzip spider.zip

# Step 3: Move it under src/data/raw/
mv spider src/data/raw/
```

---

### 2. ğŸ› ï¸ Install Dependencies

```bash
pip install -r requirements.txt
```

---

### 3. ğŸ§  Vector Database Setup

Build ChromaDB embeddings from your Spider dataset:

```bash
python setup.py
```

This will index all database schemas and sample data. It may take 10â€“15 minutes.

---

## ğŸ” HuggingFace / OpenAI API Key

Create a `.env` file in the root directory:

```bash
touch .env
```

Add the following:

```
hf_token=your_huggingface_api_key
OPENAI_API_KEY=your_openai_key_if_used
```

---

## ğŸ§ª Running the Application

### âœ³ï¸ Run Backend (FastAPI)

```bash
cd src/app
uvicorn backend:app --reload --host 0.0.0.0 --port 8000
```

### ğŸ¨ Run Frontend (Streamlit)

In a new terminal:

```bash
cd src/app
streamlit run frontend.py
```

Then open: [http://localhost:8501](http://localhost:8501)

---

## âœï¸ Example Workflow

1. Open the frontend UI.
2. Enter a question like:  
   _â€œHow many heads of the departments are older than 56?â€_
3. It will:
   - Find relevant schemas
   - Generate a SQL query
   - Execute the query
   - Return a clean, formatted answer

---

## ğŸ“ Citation & Acknowledgements

- **LangChain**  
  Chase, H. (2022). [LangChain GitHub](https://github.com/hwchase17/langchain)

- **Spider Dataset**  
  Yu, T., et al. (2018). Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing. [arXiv:1809.08887](https://arxiv.org/abs/1809.08887)

---


